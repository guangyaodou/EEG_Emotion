{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3747405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2622db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY if necessary\n",
    "subject_id = \"02\"\n",
    "\n",
    "number_of_folds = 10\n",
    "\n",
    "# If true, we ignore neural case and only calssfiy between positive and negative\n",
    "binary_classification = True\n",
    "\n",
    "number_of_neighbors = 5\n",
    "\n",
    "\n",
    "# FOR POWER DATA:\n",
    "# Frequency bands for power data\n",
    "power_band = [\"alpha\", \"beta\", \"delta\", \"theta\"]\n",
    "# the channel you want to add for the power data\n",
    "channel = [\"AF7\", 'AF8']\n",
    "\n",
    "NORMALIZE_METHODS = \"standard_scalar\",  # normalizer, \"standard_scalar\", \"MinMax\", or None\n",
    "\n",
    "\n",
    "# DO NOT MODIFY\n",
    "lower_bound = 10\n",
    "# Because we are cutting the length of each unit to match the length of power\n",
    "upper_bound = 550 \n",
    "power_length = 153\n",
    "\n",
    "NUM_OF_POWER_COLUMNS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_units_to_folds(df, folds, lower_bound, upper_bound):\n",
    "    \"\"\"\n",
    "    Execute before k-fold cross-validation: extract each unit, assign it to different folds in order\n",
    "    df: The dataframe that contains data of each patient\n",
    "    lower_bound: Experiment time's lower bound indicating the beginning of each unit\n",
    "    upper_bound: Experiment time's upper bound indicating the ending of each unit\n",
    "    \"\"\"\n",
    "    num_folds = len(folds)\n",
    "    num_rows = len(df)\n",
    "    j = 1\n",
    "    i = fold_pointer = 0\n",
    "    while j <= num_rows - 1:\n",
    "        prev_time = df.iloc[j-1][\"Time\"]\n",
    "        time = df.iloc[j][\"Time\"]\n",
    "        # if the time jumps from upper_bound(250) to a time smaller than the lower_bound (-80)\n",
    "        # we find a unit\n",
    "        if(time < lower_bound and prev_time > upper_bound):\n",
    "            unit = df.iloc[i:j]\n",
    "            folds[fold_pointer].append(unit)\n",
    "            fold_pointer = (fold_pointer + 1) % num_folds\n",
    "            i = j\n",
    "        j = j + 1\n",
    "    last_unit = df.iloc[i : j]\n",
    "    folds[fold_pointer].append(last_unit)\n",
    "    \n",
    "def concat_dataframes(fold_list, remove_columns_names):\n",
    "    \"\"\"\n",
    "    concatenate lists of dataframes to one dataframe and drop the specified columns if needed\n",
    "    fold_list: a list of folds \n",
    "    remove_columns_names: a list of names of columns you want to exclude\n",
    "    \"\"\"\n",
    "    folds_concat = []\n",
    "    for fold in fold_list:\n",
    "        folds_concat.append(pd.concat(fold, ignore_index=True).drop(columns=remove_columns_names))\n",
    "    return folds_concat\n",
    "\n",
    "def get_features(subject_data):\n",
    "    column_names = subject_data.columns\n",
    "    alpha_columns = [i for i in column_names if \"alpha\" in i]\n",
    "    beta_columns = [i for i in column_names if \"beta\" in i]\n",
    "    theta_columns = [i for i in column_names if \"theta\" in i]\n",
    "    \n",
    "    alpha = subject_data.loc[:, alpha_columns]\n",
    "    beta = subject_data.loc[:, beta_columns]\n",
    "    theta = subject_data.loc[:, theta_columns]\n",
    "    \n",
    "    alpha_std = np.std(alpha, axis=1)\n",
    "    beta_std = np.std(beta, axis=1)\n",
    "    theta_std = np.std(theta, axis=1)\n",
    "    \n",
    "    alpha_mean = np.mean(alpha, axis=1)\n",
    "    beta_mean = np.mean(beta, axis=1)\n",
    "    theta_mean = np.mean(theta, axis=1)\n",
    "    \n",
    "    #Concate feature\n",
    "    feature = np.array([theta_std,theta_mean,alpha_std,alpha_mean,beta_std,beta_mean])\n",
    "    feature = feature.T\n",
    "\n",
    "    return feature\n",
    "\n",
    "def match_power(df, df_power, index):\n",
    "    \"\"\"\n",
    "    df: The frequency dataframe\n",
    "    df_power: The power dataframe\n",
    "    index: the starting index of power name\n",
    "    \"\"\"\n",
    "    num_columns = len(df_power.columns)\n",
    "    assert num_columns == NUM_OF_POWER_COLUMNS\n",
    "    res = pd.DataFrame()\n",
    "    zero_idx = df.index[df['Time'] == 0].tolist()\n",
    "    for idx in zero_idx:\n",
    "        df_temp = df.iloc[idx:idx + power_length + 1, :]\n",
    "        res = pd.concat([res, df_temp], axis=0)\n",
    "    res = res.reset_index()\n",
    "    assert len(res) == len(df_power)\n",
    "    column_names = [f\"Power{i}\" for i in range(index, index+num_columns)]\n",
    "    df_power = df_power.set_axis(column_names, axis=1)\n",
    "           \n",
    "    df_res = pd.concat([res, df_power], axis=1)\n",
    "    return df_res\n",
    "\n",
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.rcParams.update({'font.size': 5})\n",
    "    plt.tick_params(axis='y', labelsize=5)\n",
    "    plt.savefig(f'output/subject_{subject_id}/{subject_id}_LSVM_FeatureImportance.png', dpi=2000, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c20eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read negative, neutral, and positive data from data folder\n",
    "states = [\"negative\", \"neutral\", \"positive\"]\n",
    "bands = [\"alpha\", \"beta\", \"delta\", \"theta\"]\n",
    "neg_neu_pos = []\n",
    "    \n",
    "for state in states:\n",
    "    df = pd.DataFrame()\n",
    "    for band in bands:\n",
    "        df_temp = pd.read_csv(f\"data_newcut/subject_{subject_id}/{subject_id}_processed_{state}_flt_{band}.csv\")\n",
    "        df_temp = df_temp.rename(columns={\"TP9\":f\"TP9_{band}\",\n",
    "                                          \"AF7\":f\"AF7_{band}\",\n",
    "                                          \"AF8\":f\"AF8_{band}\",\n",
    "                                          \"TP10\":f\"TP10_{band}\"})\n",
    "        # remove the time column for beta and theta\n",
    "        if band in [\"beta\", \"theta\", \"delta\"]:\n",
    "            df_temp = df_temp.drop([f\"Time\"], axis=1)\n",
    "        \n",
    "        df = pd.concat([df, df_temp], axis=1)\n",
    "        \n",
    "    # Remove times that are smaller than 0\n",
    "    df = df[df['Time'] >= 0].reset_index(drop=True)\n",
    "    neg_neu_pos.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c634d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# states are the same as the previous ones: \"negative\", \"neutral\", \"positive\"\n",
    "for i, state in enumerate(states):\n",
    "    s_state = neg_neu_pos[i]\n",
    "    power_idx = 1\n",
    "    for band in power_band:\n",
    "        for cn in channel:\n",
    "            tmp_power = pd.read_csv(f\"data_newcut/Power/subject_{subject_id}/{subject_id}_processed_{state}_flt_{band}_{cn}_power.csv\", header = None)\n",
    "            tmp_power = tmp_power.transpose()\n",
    "\n",
    "            s_state = match_power(s_state, tmp_power, power_idx)\n",
    "            lst_column = [i for i in range(power_idx, power_idx + NUM_OF_POWER_COLUMNS)]\n",
    "            s_state = s_state.drop(columns = [\"index\"])\n",
    "            \n",
    "            # Add average of the previous five power columns\n",
    "            s_state[f\"avg_{band}_power_{cn}\"] = s_state.iloc[:, -NUM_OF_POWER_COLUMNS:].sum(axis=1) / NUM_OF_POWER_COLUMNS\n",
    "            power_idx = power_idx + NUM_OF_POWER_COLUMNS\n",
    "            \n",
    "    neg_neu_pos[i] = s_state\n",
    "subject_negative = neg_neu_pos[0]\n",
    "subject_neutral = neg_neu_pos[1]\n",
    "subject_positive  = neg_neu_pos[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d4035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# psd = pd.read_csv(f\"data_newcut/subject_{subject_id}/{subject_id}_PSD.csv\", header = None)\n",
    "# psd = psd.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8fba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose negative = 0; positive = 1; and neutral = 2; \n",
    "subject_negative[\"y\"] = 0\n",
    "subject_neutral[\"y\"] = 2\n",
    "subject_positive[\"y\"] = 1\n",
    "\n",
    "# Concatenate all three datasets\n",
    "subject_data = pd.concat([subject_negative, subject_neutral, subject_positive], ignore_index=True)\n",
    "\n",
    "subject_data = subject_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcfd68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = get_features(subject_data)\n",
    "df_newFeature = pd.DataFrame(feature, columns = ['theta_std','theta_mean','alpha_std',\n",
    "                                                 'alpha_mean','beta_std','beta_mean'\n",
    "                                                ])\n",
    "df = pd.concat([subject_data, df_newFeature], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa00476",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_column = df.pop(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a6829",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns[df.columns != 'Time']\n",
    "if NORMALIZE_METHODS == \"normalizer\":\n",
    "    df[cols] = sklearn.preprocessing.normalize(df[cols], axis=0)\n",
    "elif NORMALIZE_METHODS == \"standard_scalar\":\n",
    "    df[cols] = sklearn.preprocessing.scale(df[cols]),\n",
    "elif NORMALIZE_METHODS == \"MinMax\":\n",
    "    df[cols] = sklearn.preprocessing.MinMaxScaler().fit_transform(df[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431d80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df, \n",
    "             columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83593c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"y\"] = y_column.replace(np.nan, 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc424b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if binary_classification:\n",
    "    df = df[df['y'] != 2]\n",
    "\n",
    "folds = [[] for i in range(number_of_folds)]\n",
    "assign_units_to_folds(df, folds, lower_bound, upper_bound)\n",
    "columns = df.columns\n",
    "\n",
    "columns_to_remove = []\n",
    "columns_to_remove.append(columns[0])\n",
    "columns_to_remove.extend([i for i in columns if \"TP\" in i])\n",
    "# columns_to_remove.extend(columns[0:13])\n",
    "print(\"Columns removed\", columns_to_remove)\n",
    "folds_concat = concat_dataframes(folds, columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97170931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMs take long time to run!\n",
    "names = [\n",
    "#     'XGBoost',\n",
    "#     \"Adaboost\",\n",
    "#     \"RandomForest\",\n",
    "#     \"GradientBoost\",\n",
    "#     'Nearest Neighbors',\n",
    "#     'LDA',\n",
    "#     'RBF SVM',\n",
    "    \"Linear SVM\",\n",
    "]\n",
    "\n",
    "acc_res = {}\n",
    "for name in names:\n",
    "    accuracy_lst = []\n",
    "    for _ in range(number_of_folds):\n",
    "        if name == \"Adaboost\":\n",
    "            clf = AdaBoostClassifier()\n",
    "        if name == \"RandomForest\":\n",
    "            clf = RandomForestClassifier(n_estimators=100, max_features=\"sqrt\", oob_score=True)\n",
    "        if name == \"GradientBoost\":\n",
    "            clf = GradientBoostingClassifier()\n",
    "        if name == 'Nearest Neighbors':\n",
    "            clf = KNeighborsClassifier(n_neighbors=number_of_neighbors)\n",
    "        if name == \"LDA\":\n",
    "            clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "        if name == \"RBF SVM\":\n",
    "            clf = SVC(gamma=2, C=1)\n",
    "        if name == \"Linear SVM\":\n",
    "            clf = SVC(kernel=\"linear\", C=0.025)\n",
    "        if name == \"XGBoost\":\n",
    "            clf = xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric='mlogloss', random_state=42, use_label_encoder=False)\n",
    "        train_data = pd.concat(folds_concat[:-1], ignore_index=True)\n",
    "        # take the last fold as the test set\n",
    "        test_data = folds_concat[-1]\n",
    "        # move the last fold to the beginning of the list of folds\n",
    "        folds_concat = folds_concat[-1:] + folds_concat[:-1]\n",
    "        train_X = train_data.iloc[:, :-1]\n",
    "        train_Y = train_data.iloc[:, -1]\n",
    "        test_X = test_data.iloc[:, :-1]\n",
    "        test_Y = test_data.iloc[:, -1]\n",
    "        clf.fit(train_X, train_Y)\n",
    "        y_predict = clf.predict(test_X)\n",
    "        accuracy = metrics.accuracy_score(y_predict,test_Y)\n",
    "        accuracy_lst.append(accuracy)\n",
    "    if name == \"Linear SVM\":\n",
    "        f_importances(clf.coef_[0], clf.feature_names_in_)\n",
    "    avg_acc = round(sum(accuracy_lst) / len(accuracy_lst),3)\n",
    "    acc_res[name] = avg_acc\n",
    "    print(f\"{name} yields accuracy of {avg_acc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
